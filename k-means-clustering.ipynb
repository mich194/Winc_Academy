{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://static.wincacademy.nl/logos/main-logo.png\" height=200px style=\"height: 200px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib --user > /dev/null\n",
    "!pip install numpy --user > /dev/null\n",
    "!pip install sklearn --user > /dev/null\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "Here's a dataset containing measurements of different plants. We load the data from a library called *sklearn* and print the decription they provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the description above, answer the following questions:\n",
    "\n",
    "**1. Which plant species are included in this dataset?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**2. Which properties of these species are included in this dataset?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**3. How many data points do we have in total, and how many per plant species?**\n",
    "\n",
    "*TODO: Your answer here.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assign the data and the matching classification to different variables.\n",
    "\n",
    "- The measurements we have is our *data*.\n",
    "- The matching classification -- in this case a plant species -- is known as a *target*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = iris['data'], iris['target']\n",
    "print(f'First 10 datapoints:\\n{data[:10]}\\nMatching targets:\\n{targets[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What is the name of the plant species that we just printed the data from?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "It's hard to get a good feel for what the data looks like just by looking at the numbers.\n",
    "\n",
    "Let's make a plot to get a feel for what we're looking at. For each datapoint we have 4 measurements of the size of different parts of the plant. It's hard to visualise 4-dimensional data, so let's look at 2 different attributes at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16,9))\n",
    "\n",
    "setosa = data[:50]\n",
    "versicolour = data[50:100]\n",
    "virginica = data[100:]\n",
    "\n",
    "for (i, ax), (f0, f1) in zip(enumerate(axs.flatten()), combinations(list(range(4)), 2)):\n",
    "    ax.plot(setosa[:, f0], setosa[:, f1], 'o', label='Setosa')\n",
    "    ax.plot(versicolour[:, f0], versicolour[:, f1], 'o', label='Versicolour')\n",
    "    ax.plot(virginica[:, f0], virginica[:, f1], 'o', label='Virginica')\n",
    "    ax.set_xlabel(iris['feature_names'][f0])\n",
    "    ax.set_ylabel(iris['feature_names'][f1])\n",
    "    ax.set_title(f'Plot #{i+1}', loc='left')\n",
    "    ax.legend()\n",
    "    \n",
    "fig.suptitle('Pairs of Iris subspecies attribute measurements');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn from the plots that not all attributes separate the different subspecies as well as others.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "**5. What does this mean, in your own words?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**6. How can we learn this from the plots?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "Imagine we didn't have the labels. In other words, we only have the sepal and petal measurements, and no clue about which plant species they belong to. You can see what the plots would look like below in the plot titled \"Pairs of Iris subspecies attribute measurements without labels\".\n",
    "\n",
    "**7. In this case, which pair of measurements should we use to draw new groups from scratch? In other words: which plot would you use to re-colorize the dots in a way that approaches the original distribution? Why?**\n",
    "\n",
    "*TODO: Your answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16,9))\n",
    "\n",
    "for (i, ax), (f0, f1) in zip(enumerate(axs.flatten()), combinations(list(range(4)), 2)):\n",
    "    ax.plot(data[:, f0], data[:, f1], 'o')\n",
    "    ax.set_xlabel(iris['feature_names'][f0])\n",
    "    ax.set_ylabel(iris['feature_names'][f1])\n",
    "    ax.set_title(f'Plot #{i+1}', loc='left')\n",
    "    \n",
    "fig.suptitle('Pairs of Iris subspecies attribute measurements without labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "The task described in question 6 is called *clustering*. It's about finding clusters of data points that belong together. Here are some examples of when it's used:\n",
    "\n",
    "- A company has a database of customers. They want to divide their customers into 5 groups in order to understand them better and adjust their plans accordingly.\n",
    "- A social network would like to automatically divide their users into groups so that advertising agencies can select them as their target audience when they buy an ad with the social network.\n",
    "- A chemical lab would like to automatically identify unknown substances.\n",
    "- A botanist would like to classify plants into different species based on measurements they did.\n",
    "\n",
    "Here we will pretend to work on that last case. We are the botanists, and we have plant measurements *without labels*. It is our job to draw clusters of unique plants.\n",
    "\n",
    "**8. How would you solve this problem if you had the dots on a piece of paper? Explain your intuitive approach in a number of steps. Be precise! If you want to say \"draw a circle around dots that are close\", ask yourself: what is 'close'? How many circles will you draw? How many dots should be in each circle?**\n",
    "\n",
    "*TODO: Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-means is an unsupervised machine learning clustering algorithm.\n",
    "\n",
    "\n",
    "- *Unsupervised* means the method does not rely on a dataset of known outcomes; $k$-means clusters datapoints without using labeled examples of 'correctly clustered' datapoints.\n",
    "- *Machine learning* is the concept of programming a computer to find a pattern for us, instead of finding the pattern ourselves.\n",
    "- *Clustering* is the task described in the previous section.\n",
    "- An *algorithm* is a series of steps to perform to accomplish a task.\n",
    "\n",
    "Here's how it works on an intuitive level:\n",
    "\n",
    "1. Choose the number of clusters, $k$, and a threshold.\n",
    "2. Pick $k$ unique starting positions. These positions are the *centroids*.\n",
    "3. For each datapoint, calculate the distance to each centroid.\n",
    "4. **Assign**: assign each datapoint to the nearest centroid. The groups that form are the clusters.\n",
    "5. **Update**. For each cluster, update the centroid by computing the *mean* of all the datapoint in it. This mean is the new centroid.\n",
    "6. Loop the **assign** $\\leftrightarrow$ **update** steps until the centroids' movement falls below the threshold.\n",
    "\n",
    "Here's what the implementation looks like for our plant data. We will use the *petal length* and *petal width* features of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_means_clustering(data, k, threshold, max_iterations=100, plots=False):\n",
    "    delta = float('inf')\n",
    "    \n",
    "    # 2. Pick k unique starting positions\n",
    "    unique_data = np.unique(data, axis=0)\n",
    "    centroid_indices = np.random.randint(0, unique_data.shape[0], k)\n",
    "    while len(set(centroid_indices)) != k:\n",
    "        centroid_indices = np.random.randint(0, unique_data.shape[0], k)\n",
    "    new_centroids = unique_data[centroid_indices]\n",
    "    del unique_data\n",
    "        \n",
    "    iterations = 0\n",
    "    # While the centroids are moving more than some predefined value \n",
    "    while delta > threshold and iterations < max_iterations:\n",
    "        clusters = [[] for _ in range(k)]\n",
    "    \n",
    "        # 3. For each datapoint...\n",
    "        for d in data:\n",
    "            # ... calculate the distance to each centroid ...\n",
    "            norms = [np.linalg.norm(d - c) for c in new_centroids]\n",
    "            # 4. ... and assign the datapoint to the nearest centroid's cluster\n",
    "            clusters[np.argmin(norms)].append(d)\n",
    "        clusters = np.array([np.array(x) for x in clusters], dtype=object)\n",
    "        \n",
    "        if plots:\n",
    "            # Extra: plot what our clusters look like at this moment.\n",
    "            fig, ax = plt.subplots(figsize=(8,6))\n",
    "            fig.suptitle(f'Iteration {iterations}')\n",
    "            for (j, cluster), centroid in zip(enumerate(clusters), new_centroids):\n",
    "                color = ax.plot(cluster[:,0], cluster[:,1], '.', label=f'cluster {j}')[0].get_color()\n",
    "                ax.plot(centroid[0], centroid[1], '^', label=f\"cluster {j}'s centroid\", color=color)\n",
    "            ax.set_xlabel(iris['feature_names'][2])\n",
    "            ax.set_ylabel(iris['feature_names'][3])\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # 5. Update the centroid\n",
    "        old_centroids = new_centroids\n",
    "        new_centroids = [np.mean(c, axis=0) for c in clusters]\n",
    "    \n",
    "        # 6. Check how much the centroids moved.\n",
    "        delta = np.linalg.norm(np.array(new_centroids, dtype=object) - np.array(old_centroids, dtype=object))\n",
    "        iterations += 1\n",
    "    return iterations, clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2d = data[:,2:]\n",
    "\n",
    "# ADJUST THE VARIABLE BELOW TO PLAY WITH THE NUMBER OF CLUSTERS\n",
    "k = 2\n",
    "# DON'T MODIFY ANYTHING BELOW THIS LINE\n",
    "\n",
    "iterations, clusters = k_means_clustering(data_2d, k, threshold=0.05, plots=True)\n",
    "print(f'Executed in {iterations} iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. The algorithm was set to find 2 clusters ($k=2$). Does it makes sense to keep $k$ at this level? Why/why not? If not, what would you change it to?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**10. Run the algorithm a few times with the right value for $k$. Note the number of iterations used each time. Is it the same? Why/why not?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**11. Compare the plots after each iteration. Why are the centroids moving and the clusters changing?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**11. Is it guaranteed that for a particular value of $k$ we find the same clusters every time we run it? Why/why not?**\n",
    "\n",
    "*TODO: Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating K-Means\n",
    "\n",
    "We now have an algorithm for finding clusters in our plant data. But how good is it? Let's compare some results from k-means clustering to the real, ground-truth labels that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(16,18))\n",
    "\n",
    "axs[0].plot(setosa[:, 2], setosa[:, 3], 'o', label='Setosa')\n",
    "axs[0].plot(versicolour[:, 2], versicolour[:, 3], 'o', label='Versicolour')\n",
    "axs[0].plot(virginica[:, 2], virginica[:, 3], 'o', label='Virginica')\n",
    "axs[0].set_xlabel(iris['feature_names'][2])\n",
    "axs[0].set_ylabel(iris['feature_names'][3])\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Ground truth')\n",
    "\n",
    "_, clusters = k_means_clustering(data_2d, 3, threshold=0.05, plots=False)\n",
    "axs[1].plot(clusters[0][:, 0], clusters[0][:, 1], 'o', label='Cluster 0')\n",
    "axs[1].plot(clusters[1][:, 0], clusters[1][:, 1], 'o', label='Cluster 1')\n",
    "axs[1].plot(clusters[2][:, 0], clusters[2][:, 1], 'o', label='Cluster 2')\n",
    "axs[1].set_xlabel(iris['feature_names'][2])\n",
    "axs[1].set_ylabel(iris['feature_names'][3])\n",
    "axs[1].legend()\n",
    "axs[1].set_title('k-means result');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. How well did k-means algorithm do? Compare its result with the ground truth. Where did k-means go wrong, and why do you think it went wrong there?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**13. How would you evaluate k-means clustering if you didn't have ground truth data? What does your answer say about using k-means in real life?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "**14. Below you see data that is shaped like a mouse. On the left are the real labels. In the middle is a k-means clustering result. On the right you see the result from a different clustering algorithm (EM). Which one did better? Why did the k-means clustering attempt turn out this way?**\n",
    "\n",
    "*TODO: Your answer here.*\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/2560px-ClusterAnalysis_Mouse.svg.png\"/>\n",
    "\n",
    "In this lesson you learned about the k-means clustering algorithm. If you use a data analytics tool in the future that offers it as a tool to perform clustering, now you'll know how it works under the hood and what its limitations are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
